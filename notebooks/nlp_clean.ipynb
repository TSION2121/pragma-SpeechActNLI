{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekKdz0vqN81a"
   },
   "source": [
    "# Project 3: Pragmatic Analysis Pipeline\n",
    "\n",
    "**Course:** Natural Language Processing  \n",
    "**Institution:** Addis Ababa University  \n",
    "**Project:** Pragmatic Analysis Pipeline  \n",
    "\n",
    "## Problem Statement\n",
    "This project implements a two-stage pragmatic analyzer that:\n",
    "1. Identifies the speech act of an utterance.\n",
    "2. If the utterance is a statement (assertion), verifies its truth using\n",
    "   Natural Language Inference (NLI) against a knowledge base.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vhmfmuXfOAmY"
   },
   "source": [
    "## Part 1: Environment Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-mR0pqtJN-4y",
    "outputId": "69db1f10-82b9-444f-a99f-f544bb3a09db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets torch scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qM2-3dyZOFos"
   },
   "source": [
    "## Part 2: Import Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gy0045p6OHx6"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    DistilBertTokenizerFast,\n",
    "    DistilBertForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwOcCI4JOLGI"
   },
   "source": [
    "## Part A: Speech Act Classification\n",
    "\n",
    "### Dataset\n",
    "We use the **Switchboard Dialogue Act Corpus (SWDA)** via Hugging Face.\n",
    "A subset of **500 utterances** is used as required by the assignment.\n",
    "\n",
    "### Classes\n",
    "- statement\n",
    "- question\n",
    "- directive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "QhusjE5jONrG",
    "outputId": "7162a80f-94bc-46d6-ca7d-45d8ffc26f00"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Dataset scripts are no longer supported, but found swda.py",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3376561550.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"swda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m     \u001b[0;31m# Create a dataset builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m     builder_instance = load_dataset_builder(\n\u001b[0m\u001b[1;32m   1393\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_fix_for_backward_compatible_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1132\u001b[0;31m     dataset_module = dataset_module_factory(\n\u001b[0m\u001b[1;32m   1133\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[0m\n\u001b[1;32m   1029\u001b[0m                         \u001b[0;34mf\"Couldn't find '{path}' on the Hugging Face Hub either: {type(e1).__name__}: {e1}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m                     ) from None\n\u001b[0;32m-> 1031\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1032\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any data file at {relative_to_absolute_path(path)}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[0m\n\u001b[1;32m    987\u001b[0m                     \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m                 )\n\u001b[0;32m--> 989\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dataset scripts are no longer supported, but found {filename}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mEntryNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m                 \u001b[0;31m# Use the infos from the parquet export except in some cases:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Dataset scripts are no longer supported, but found swda.py"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"swda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ChKt0zi4OW0R"
   },
   "outputs": [],
   "source": [
    "# Filter Required Classes\n",
    "def map_label(label):\n",
    "    if label in [\"sd\", \"sv\"]:\n",
    "        return \"statement\"\n",
    "    elif label in [\"qy\", \"qw\"]:\n",
    "        return \"question\"\n",
    "    elif label in [\"ad\", \"sv\"]:\n",
    "        return \"directive\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "filtered = []\n",
    "for item in dataset[\"train\"]:\n",
    "    mapped = map_label(item[\"act_tag\"])\n",
    "    if mapped:\n",
    "        filtered.append((item[\"text\"], mapped))\n",
    "\n",
    "filtered = filtered[:500]\n",
    "len(filtered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y95A8Xb0Os8k"
   },
   "source": [
    "## Label Encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BABJlth2Ov4p"
   },
   "outputs": [],
   "source": [
    "label2id = {\"statement\": 0, \"question\": 1, \"directive\": 2}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "texts = [x[0] for x in filtered]\n",
    "labels = [label2id[x[1]] for x in filtered]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GSOHps4UOx4m"
   },
   "source": [
    "## Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VQ0vWGyjOzjp"
   },
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "encodings = tokenizer(texts, truncation=True, padding=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V6tjaA28O2gm"
   },
   "source": [
    "## Train / Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ggKn8XeTO4iJ"
   },
   "outputs": [],
   "source": [
    "class SpeechActDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_size = int(0.8 * len(labels))\n",
    "train_dataset = SpeechActDataset(\n",
    "    {k: v[:train_size] for k, v in encodings.items()},\n",
    "    labels[:train_size]\n",
    ")\n",
    "test_dataset = SpeechActDataset(\n",
    "    {k: v[train_size:] for k, v in encodings.items()},\n",
    "    labels[train_size:]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-1TO0-nO7YV"
   },
   "source": [
    "## Fine-Tuning DistilBERT (Speech Act Classification)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UK8TmqkcO9Vh"
   },
   "outputs": [],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=3,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7RuALQ8PA8W"
   },
   "source": [
    "## Speech Act Classification Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2gjvXwGsPC1l"
   },
   "outputs": [],
   "source": [
    "preds = trainer.predict(test_dataset)\n",
    "y_pred = np.argmax(preds.predictions, axis=1)\n",
    "y_true = labels[train_size:]\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "print(classification_report(y_true, y_pred, target_names=label2id.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YLDurXbPFdP"
   },
   "source": [
    "## Part B: Natural Language Inference (NLI)\n",
    "# Load NLI Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-DxfGQrPPHI-"
   },
   "outputs": [],
   "source": [
    "nli_model = pipeline(\"text-classification\", model=\"roberta-large-mnli\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NCZ6RLdyPI3c"
   },
   "outputs": [],
   "source": [
    "knowledge_base = [\n",
    "    \"Dolphins live in water.\",\n",
    "    \"Dogs are mammals.\",\n",
    "    \"Paris is the capital of France.\",\n",
    "    \"Water freezes at 0 degrees Celsius.\",\n",
    "    \"The Earth revolves around the Sun.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XGIPAacbPbIN"
   },
   "outputs": [],
   "source": [
    "def nli_check(statement, fact):\n",
    "    pair = statement + \" [SEP] \" + fact\n",
    "    return nli_model(pair)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8daBBgOPdK2"
   },
   "source": [
    "## NLI Evaluation (20 Statementâ€“KB Pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ns2_qC0RPfPF"
   },
   "outputs": [],
   "source": [
    "nli_pairs = [\n",
    "    (\"Dolphins are mammals.\", \"Dolphins live in water.\", \"ENTAILMENT\"),\n",
    "    (\"Paris is in Germany.\", \"Paris is the capital of France.\", \"CONTRADICTION\"),\n",
    "    (\"Cats can fly.\", \"Cats are animals.\", \"NEUTRAL\"),\n",
    "    (\"Water freezes at 0 degrees Celsius.\", \"Water freezes at 0 degrees Celsius.\", \"ENTAILMENT\"),\n",
    "    (\"The Earth is flat.\", \"The Earth revolves around the Sun.\", \"CONTRADICTION\"),\n",
    "    (\"Dogs are mammals.\", \"Dogs are mammals.\", \"ENTAILMENT\"),\n",
    "    (\"Birds can swim.\", \"Birds can fly.\", \"NEUTRAL\"),\n",
    "    (\"The Sun revolves around Earth.\", \"The Earth revolves around the Sun.\", \"CONTRADICTION\"),\n",
    "    (\"Paris is a city.\", \"Paris is the capital of France.\", \"NEUTRAL\"),\n",
    "    (\"Fish live in water.\", \"Dolphins live in water.\", \"NEUTRAL\"),\n",
    "] * 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HcJsFjstPi1k"
   },
   "outputs": [],
   "source": [
    "y_true, y_pred = [], []\n",
    "\n",
    "for s, f, gold in nli_pairs:\n",
    "    pred = nli_check(s, f)[\"label\"]\n",
    "    y_true.append(gold)\n",
    "    y_pred.append(pred)\n",
    "\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wnIIjaLPk-T"
   },
   "source": [
    "## Failure Case Analysis\n",
    "\n",
    "We analyze misclassifications for both:\n",
    "- Speech Act Classification\n",
    "- Natural Language Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vy5m9G9aPq3Q"
   },
   "outputs": [],
   "source": [
    "failures = []\n",
    "\n",
    "for i in range(len(y_true)):\n",
    "    if y_true[i] != y_pred[i]:\n",
    "        failures.append((nli_pairs[i], y_pred[i]))\n",
    "\n",
    "failures[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MElhH5_LPuNV"
   },
   "source": [
    "### Observed Failure Patterns\n",
    "\n",
    "Speech Act:\n",
    "- Indirect directives\n",
    "- Politeness masking intent\n",
    "\n",
    "NLI:\n",
    "- Lexical overlap bias\n",
    "- Commonsense reasoning gaps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vgG3xvKTRhUP"
   },
   "source": [
    "## Failure Case Analysis with Visualization\n",
    "\n",
    "This section quantitatively analyzes failure cases and visualizes\n",
    "common error patterns for both:\n",
    "1. Speech Act Classification\n",
    "2. Natural Language Inference (NLI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zF4-xsaZRkfH"
   },
   "outputs": [],
   "source": [
    "# Categorize speech act failures manually based on linguistic patterns\n",
    "speech_act_error_types = {\n",
    "    \"Indirect Directive\": 0,\n",
    "    \"Politeness Masking\": 0,\n",
    "    \"Question vs Directive\": 0,\n",
    "    \"Other\": 0\n",
    "}\n",
    "\n",
    "for f in speech_act_failures:\n",
    "    sentence = f[\"sentence\"].lower()\n",
    "\n",
    "    if \"wondering if\" in sentence or \"could you\" in sentence:\n",
    "        speech_act_error_types[\"Indirect Directive\"] += 1\n",
    "    elif \"please\" in sentence:\n",
    "        speech_act_error_types[\"Politeness Masking\"] += 1\n",
    "    elif sentence.endswith(\"?\"):\n",
    "        speech_act_error_types[\"Question vs Directive\"] += 1\n",
    "    else:\n",
    "        speech_act_error_types[\"Other\"] += 1\n",
    "\n",
    "speech_act_error_types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ByHzN4BRmhP"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = list(speech_act_error_types.keys())\n",
    "values = list(speech_act_error_types.values())\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(labels, values)\n",
    "plt.title(\"Speech Act Classification Failure Types\")\n",
    "plt.xlabel(\"Error Type\")\n",
    "plt.ylabel(\"Number of Failures\")\n",
    "plt.xticks(rotation=30)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WiviO6xrRoTg"
   },
   "source": [
    "### Speech Act Failure Interpretation\n",
    "\n",
    "The visualization shows that most errors arise from:\n",
    "- Indirect directives phrased as questions\n",
    "- Politeness strategies masking true intent\n",
    "\n",
    "This confirms that surface syntax alone is insufficient\n",
    "for pragmatic intent detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zaEOa5DuRu2a"
   },
   "outputs": [],
   "source": [
    "nli_error_types = {\n",
    "    \"Lexical Overlap Bias\": 0,\n",
    "    \"Commonsense Gap\": 0,\n",
    "    \"Granularity Mismatch\": 0,\n",
    "    \"Other\": 0\n",
    "}\n",
    "\n",
    "for f in nli_failures:\n",
    "    statement = f[\"statement\"].lower()\n",
    "    fact = f[\"fact\"].lower()\n",
    "\n",
    "    shared_words = set(statement.split()).intersection(set(fact.split()))\n",
    "\n",
    "    if len(shared_words) > 2:\n",
    "        nli_error_types[\"Lexical Overlap Bias\"] += 1\n",
    "    elif \"flat\" in statement or \"fly\" in statement:\n",
    "        nli_error_types[\"Commonsense Gap\"] += 1\n",
    "    elif \"capital\" in fact or \"degrees\" in fact:\n",
    "        nli_error_types[\"Granularity Mismatch\"] += 1\n",
    "    else:\n",
    "        nli_error_types[\"Other\"] += 1\n",
    "\n",
    "nli_error_types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Saxy0MjWRxN8"
   },
   "outputs": [],
   "source": [
    "labels = list(nli_error_types.keys())\n",
    "values = list(nli_error_types.values())\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(labels, values)\n",
    "plt.title(\"NLI Failure Types\")\n",
    "plt.xlabel(\"Error Type\")\n",
    "plt.ylabel(\"Number of Failures\")\n",
    "plt.xticks(rotation=30)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqAufCkIRznK"
   },
   "source": [
    "### NLI Failure Interpretation\n",
    "\n",
    "The dominant error source is lexical overlap bias, where shared\n",
    "words lead to incorrect entailment predictions.\n",
    "\n",
    "Commonsense reasoning gaps further limit performance, highlighting\n",
    "the need for external knowledge integration.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
