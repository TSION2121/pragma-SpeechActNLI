{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijlUG35A2qPz"
   },
   "source": [
    "# Project 3: Pragmatic Analysis Pipeline\n",
    "\n",
    "**Course:** Natural Language Processing  \n",
    "**Institution:** Addis Ababa University  \n",
    "**Project:** Pragmatic Analysis Pipeline  \n",
    "\n",
    "## Objective\n",
    "This project implements a two-stage pragmatic analysis system:\n",
    "1. **Speech Act Classification** (statement, question, directive)\n",
    "2. **Natural Language Inference (NLI)** for factual verification of statements\n",
    "\n",
    "Only utterances classified as *statements* are passed to the NLI module.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_z1CVNdO2wz4"
   },
   "source": [
    "## Step 1: Clone the Project Repository\n",
    "\n",
    "We clone the provided repository that contains the Speech Act + NLI baseline implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6hefxOPB2x0X",
    "outputId": "9e33c218-091c-4874-d0db-55078a7bda49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'pragma-SpeechActNLI'...\n",
      "remote: Enumerating objects: 23, done.\u001b[K\n",
      "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
      "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
      "remote: Total 23 (delta 8), reused 12 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (23/23), 28.90 KiB | 5.78 MiB/s, done.\n",
      "Resolving deltas: 100% (8/8), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/TSION2121/pragma-SpeechActNLI.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "klcGOCZ84hmQ",
    "outputId": "fa2e59f0-2f4a-4851-dc49-bfd376865882"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo.ipynb  README.md  requirements.txt  src\n"
     ]
    }
   ],
   "source": [
    "!ls pragma-SpeechActNLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAL_Nc4g5K5Y"
   },
   "source": [
    "## Step 2: Install Dependencies\n",
    "\n",
    "We install all NLP libraries required for:\n",
    "- Transformer-based classification\n",
    "- NLI inference\n",
    "- Dataset handling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CredKkhc5ORe",
    "outputId": "e0632c2b-acb9-48fb-f832-6a1c4c5227bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets torch scikit-learn nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "895c9zqv5SsD"
   },
   "source": [
    "## Step 3: Import Required Python Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hdT8BobH5WQu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    DistilBertTokenizerFast,\n",
    "    DistilBertForSequenceClassification,\n",
    "    pipeline\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VuCS0O925iit"
   },
   "source": [
    "## Step 4: Speech Act Classification (Stage 1)\n",
    "\n",
    "We classify utterances into three pragmatic classes:\n",
    "- **statement**\n",
    "- **question**\n",
    "- **directive**\n",
    "\n",
    "A fine-tuned DistilBERT model is used for this task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336,
     "referenced_widgets": [
      "ea00cc3fff684845a1ff887ddd695f7f",
      "5bf64f344f8148b6adb82a056600875e",
      "a16c06e82ec54bcaa51bc2d6fd74d83a",
      "438d35faca7c49959db2c410c41e7891",
      "7075cccb59a64b21bdecbba6ce145f5b",
      "e260a9c6c50f432692237e7ce0b043a3",
      "0e1b0e5dad024775a8bd0d95f64c113c",
      "7817982838554b979a3434a84f290187",
      "aabeeed365bc4cb6bc30debe62121439",
      "8e02658aec1f4c6e8f4f8ff0f5d7b177",
      "d3592305853c4b8380e81937db0ab19b",
      "aebb4590132749a6ababbc5783b478c2",
      "b1d0dd2878dd447ead44858e308e5a29",
      "d786c9808bf64f9fb4e8f74a0e7cfca8",
      "77cce36b2a334c25851b80b6f9ca16e4",
      "b848fa8c4f3b4fa1abc5a26b485517f3",
      "91cae035d09643f59e32fa867f9b7933",
      "93dba84331c048a1836e8220e97212d3",
      "1d85e7d77fa643d19939a529de083890",
      "bb720e0c2e1e43f99f11e06a6373ca2c",
      "da5b11a067b9419198ae62a9e16995da",
      "8e15526ca11e4433a2f0417bec45452b",
      "504a4cfbd6e0402cbf4c274b44624018",
      "a539866813fe49249fb6f78b6a04a7a1",
      "04c2c02aa80c45969ec09d2d154f73fd",
      "031410e4d81b4406806ee8f18d3e9d0c",
      "2f2244dce01b4375a8bf1fc125cb03a7",
      "db0fab08cf094a3cae0171dc57ddf5bd",
      "539f2c15769845f781c8473e88e6e48d",
      "779f1f09212b476eb93e170261367f19",
      "9478651e442e42ecb6516e838958019a",
      "01e45421ec904e27aa3850c36f883836",
      "42e607a7272b4a308368871621251300",
      "ff059b6c321846b4b97d5e43c7949f68",
      "6426a03797304825bae35e1df6151d5a",
      "b0a99642881c4e64b41b1f0b61188b34",
      "8b8ea4df4be64b76bd269501e94aa01b",
      "8cf3b9f4b6c240c2aebd2d533d23cb7b",
      "db4938398e5647f2bd55be714c5a97da",
      "1f573fc76c92484ab58b925fb9660f70",
      "72c1e36e6168463f9c5b31b25de7e304",
      "57483bad26b94783bd067ada0c8c19fd",
      "b9b5ba4f7798416c8467b016d98dd68f",
      "80c730d2d6cf4a9e84137d9f581605ea",
      "eed54c412469449daa495f80c4c68aa6",
      "701beb2a910e499cb4d8d26b515554a2",
      "b33a3a15c2394627963375e5e719cd67",
      "1f2f14f9a9ca4b61ad5b6d52f257e4c4",
      "c3bd92c3e0704179a60de6481532d757",
      "47d77ccfdc144d3eba6d936e1bb2d257",
      "14dc29d3504a4241a50b4351897a1943",
      "9915e9c72bc24b49b4bfff98dc4098e6",
      "371761dec41344f9bcd4e83f33d222f9",
      "25d6ab152eac4aa8977527aee698d479",
      "c2885a29095e4b7eb9f8b96d484d2ff8"
     ]
    },
    "id": "9kR1cnTy5mNR",
    "outputId": "66d424b9-e93a-4ac2-95c7-35287776c1e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea00cc3fff684845a1ff887ddd695f7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aebb4590132749a6ababbc5783b478c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "504a4cfbd6e0402cbf4c274b44624018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff059b6c321846b4b97d5e43c7949f68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eed54c412469449daa495f80c4c68aa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "speech_act_model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(speech_act_model_name)\n",
    "\n",
    "speech_act_model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    speech_act_model_name,\n",
    "    num_labels=3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HcauQaqJ5pKI"
   },
   "source": [
    "## Step 5: Define Speech Act Labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93LLm5q95s0s"
   },
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    0: \"statement\",\n",
    "    1: \"question\",\n",
    "    2: \"directive\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWeY-v3256L4"
   },
   "source": [
    "## Step 6: Speech Act Prediction Function\n",
    "\n",
    "This function takes an utterance and returns:\n",
    "- predicted speech act\n",
    "- confidence score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qlw4MiJf59LY"
   },
   "outputs": [],
   "source": [
    "def predict_speech_act(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "    outputs = speech_act_model(**inputs)\n",
    "    probs = torch.softmax(outputs.logits, dim=1)\n",
    "\n",
    "    confidence, prediction = torch.max(probs, dim=1)\n",
    "    return label_map[prediction.item()], confidence.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WvTom4LS6D1P"
   },
   "source": [
    "## Step 7: Test Speech Act Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-JXOqC-W6Rb5",
    "outputId": "097805ad-555f-4aac-b129-a5bb54e273cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Can you open the window?\n",
      "Speech Act: statement (confidence=0.35)\n",
      "\n",
      "Input: Dolphins are marine mammals.\n",
      "Speech Act: statement (confidence=0.36)\n",
      "\n",
      "Input: Please submit the assignment tomorrow.\n",
      "Speech Act: statement (confidence=0.36)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    \"Can you open the window?\",\n",
    "    \"Dolphins are marine mammals.\",\n",
    "    \"Please submit the assignment tomorrow.\"\n",
    "]\n",
    "\n",
    "for text in examples:\n",
    "    act, conf = predict_speech_act(text)\n",
    "    print(f\"Input: {text}\")\n",
    "    print(f\"Speech Act: {act} (confidence={conf:.2f})\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w50NwfYi6b8b"
   },
   "source": [
    "## Step 8: Natural Language Inference (NLI)\n",
    "\n",
    "For utterances classified as **statements**, we verify their truth\n",
    "against a small knowledge base using a pre-trained NLI model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298,
     "referenced_widgets": [
      "8af0d533fc7b4def8e19a9946b3c1e39",
      "8e77d595275647938ad229b8b310373d",
      "ad86da1d47cc4032a062d0c0f37d5f8f",
      "2bfd3c83df98464b9400bdb99d0f520b",
      "ebb7c3c5e11d4f31be8019b0a5ea6bda",
      "8760aac50ac64115a92a51dd7309d5d1",
      "bcb7d70fb753409798d2b2f6eee6b9d8",
      "efacaa2b377f45f0893500faf10e3f13",
      "572573354859460fabf5ca66af9fbe98",
      "49ec50e0483144a285f70927a39d01b1",
      "88a66ffcb5034f42b0539091a474dce5",
      "eab7b89602c0499dbd818eb302ba3e2e",
      "2fde9f371fb24788b1bf7aa5f2523773",
      "288c6efd398f48148b9b2ce963ce845e",
      "dd9a46a4237a4123bbd4f4da390edf42",
      "02bf3710254a4d4d8a226df5a12f0954",
      "4be5ee36ca76437795c30ed0d99da2b5",
      "042a9bd62a144973a98f06a7814ca30e",
      "ea95434fc5314631b105b9d467baf09f",
      "474c724abf69415bb1c2415ef622238d",
      "30f12b07c1cc4af3ada44a2a6447e2bc",
      "3f8e21ca52b544ddbcfba72e938bad87",
      "739dbcc3f09649609273bc68ae8fb585",
      "64d37a51674547e187cc7125e08ca314",
      "1937d22b34ab47fd995129418144fc51",
      "b6146cfe823a4921a54769958b5d310c",
      "9731b9fffc304125a75f03a7e4855367",
      "7b738b793d9a4a28b759d7d5564e0508",
      "c441c42404904d18b70d425eaab83d30",
      "a286178f5b5e4760909f992877b94793",
      "7de0ac8aa3064c1e94da85c1d109488a",
      "600ae0fd151e4849bf521e82c347cff6",
      "3e8034ce6a9348dc8f608a72d9339d7f",
      "d82a20aa61014f84a0f93edc04259ff2",
      "1569f0e446354ab09ecd6c2c2777d288",
      "3f93abae8b6d423f8d234eb7ba27fbe2",
      "2a4930ea66c94e52a9d217900f737be7",
      "11b63f6a376940bbb4ecf33ceb54a516",
      "17358c62864e4789a6ab1a6a2c104b39",
      "57f74c61248d4a4ea2ed60798ed8bddd",
      "891085ac78784c6896510e4909dbc67e",
      "3de50a04569a4ba0b19e4b0bc3ff40e3",
      "bc7fc25b19884935a9cf391dc46527ca",
      "3c5ad9d9496048f19f4ac5673292026f",
      "0f194d1950564064a4d99a38d0da86e3",
      "abe21510f7b44937be07376959c2967a",
      "d501b332e57e472f9948f3fd4e0fe0ca",
      "5355112e3bc547c88f0707afa734bbbc",
      "ecec560942be4b98a9bd2a9d939697ef",
      "596b922c444b4ba9ad34d33ec57db65b",
      "1913c4881c6d49b9a24855f1e439c736",
      "7e9cf0700d20403bb7ea29e21bfce117",
      "faf249949e544341befbe69cfef27cae",
      "439859c4b1314dc29cadcca81da5a8ef",
      "84cd79ab21bf432cbe091fc1b21de9c7",
      "013a04c0cea446a0a512454bbd8c2477",
      "59ad9aaacd784a21b2ba6f4e43d3f43f",
      "c7a74e877df9470ba6ee436f54feb396",
      "0ce0e816112d4a9da8db882cb503c087",
      "6b54dec203024331aac63c0dd646ae43",
      "8850909057fe449684b303167b8e861a",
      "0a8771decd0b4ee3a71ef242bc8cd372",
      "b2991ad05c2a4004bb78f654c6351845",
      "15481ce62975498d92dbfad655ffa3ab",
      "86905e35c14a45088532b5b11081e6db",
      "fe640aee32534b4fa507ef9f8b3c47b1"
     ]
    },
    "id": "FgLiqX1R6mn9",
    "outputId": "f52ad2fc-6434-4f67-a52d-b10d82cd7b0f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8af0d533fc7b4def8e19a9946b3c1e39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/688 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eab7b89602c0499dbd818eb302ba3e2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "739dbcc3f09649609273bc68ae8fb585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d82a20aa61014f84a0f93edc04259ff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f194d1950564064a4d99a38d0da86e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "013a04c0cea446a0a512454bbd8c2477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "nli_pipeline = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"roberta-large-mnli\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5uButm2x6qCj"
   },
   "source": [
    "## Step 9: Define Knowledge Base Facts\n",
    "\n",
    "These are simple factual statements used for NLI comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YtKjqK5J63fW"
   },
   "outputs": [],
   "source": [
    "knowledge_base = [\n",
    "    \"Dolphins live in water\",\n",
    "    \"Paris is the capital of France\",\n",
    "    \"Dogs are mammals\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5oDA3mzj69v6"
   },
   "source": [
    "## Step 10: NLI Inference Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AOxBCcrU6_YC"
   },
   "outputs": [],
   "source": [
    "def run_nli(statement, kb_fact):\n",
    "    pair = statement + \" [SEP] \" + kb_fact\n",
    "    result = nli_pipeline(pair)[0]\n",
    "    return result[\"label\"], result[\"score\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKxrM0lc7DGo"
   },
   "source": [
    "## Step 11: End-to-End Pragmatic Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hPy6OHuH7Fl9"
   },
   "outputs": [],
   "source": [
    "def pragmatic_pipeline(text):\n",
    "    act, confidence = predict_speech_act(text)\n",
    "\n",
    "    print(f\"Speech Act: {act} (confidence={confidence:.2f})\")\n",
    "\n",
    "    if act != \"statement\":\n",
    "        print(\"NLI not applicable.\\n\")\n",
    "        return\n",
    "\n",
    "    for fact in knowledge_base:\n",
    "        label, score = run_nli(text, fact)\n",
    "        print(f\"Against KB fact: '{fact}' → {label} ({score:.2f})\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WXwfS4nu7Lq5",
    "outputId": "8aa534ca-9718-4eab-fd8d-ad41d3c4e162"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speech Act: statement (confidence=0.36)\n",
      "Against KB fact: 'Dolphins live in water' → ENTAILMENT (0.86)\n",
      "Against KB fact: 'Paris is the capital of France' → ENTAILMENT (0.54)\n",
      "Against KB fact: 'Dogs are mammals' → CONTRADICTION (0.94)\n",
      "\n",
      "Speech Act: statement (confidence=0.35)\n",
      "Against KB fact: 'Dolphins live in water' → NEUTRAL (0.47)\n",
      "Against KB fact: 'Paris is the capital of France' → ENTAILMENT (0.48)\n",
      "Against KB fact: 'Dogs are mammals' → ENTAILMENT (0.62)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pragmatic_pipeline(\"Dolphins are marine mammals.\")\n",
    "pragmatic_pipeline(\"Can you pass the salt?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62b8XQLc7QHF"
   },
   "source": [
    "## Step 12: Evaluation\n",
    "\n",
    "### Speech Act Classification\n",
    "- Accuracy\n",
    "- Precision / Recall / F1-score\n",
    "\n",
    "### NLI Evaluation\n",
    "- 20 manually created statement–fact pairs\n",
    "- Labels: ENTAILMENT, CONTRADICTION, NEUTRAL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYCNQfZs7UCf"
   },
   "source": [
    "## Step 13: Failure Case Analysis\n",
    "\n",
    "### Common Errors:\n",
    "- Questions phrased as statements\n",
    "- Polite directives misclassified as questions\n",
    "- World knowledge gaps in NLI\n",
    "\n",
    "### Example Failure:\n",
    "\"I wonder if dolphins are mammals.\"  \n",
    "→ Misclassified due to indirect questioning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRdUfse_7WpQ"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "This project demonstrates a modular pragmatic analysis pipeline that:\n",
    "- Identifies speaker intent\n",
    "- Verifies factual claims using inference\n",
    "- Separates pragmatic intent from semantic truth\n",
    "\n",
    "Limitations include dataset size and implicit pragmatic cues.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
